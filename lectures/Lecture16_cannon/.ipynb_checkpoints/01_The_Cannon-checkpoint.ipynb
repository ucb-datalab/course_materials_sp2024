{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\" integrity=\"sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "<style>\n",
       "\n",
       "@import url(https://fonts.googleapis.com/css?family=Open+Sans);body{\n",
       "   font-family: 'Open Sans';\n",
       "   font-size: 125%;\n",
       "}\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 275%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.3; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of \"The Cannon\"\n",
    "\n",
    "Astro 128/256 (UC Berkeley, 2024)\n",
    "\n",
    "From [Ness et al. (2015)](https://ui.adsabs.harvard.edu/abs/2015ApJ...808...16N/abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* A few of your stars have good labels from somewhere\n",
    "* How can you quickly and efficienctly transfer them to stars without labels?\n",
    "* Why do this?\n",
    " * No good models at wavelengths of interest\n",
    " * Two surveys on the same \"system\"\n",
    " * Stars at a variety of SNRs\n",
    " * Model-based fitting of each star prohibitively expensive (e.g., too many stars)\n",
    "* Two conceptual steps\n",
    " * Training step -- use high quality data to train and validate your model\n",
    " * Test step -- assume that the model created in the training step applies to all spectra in survey and apply the model\n",
    " * Together these two steps result in \"label transfer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Select a Training Set (Sec 2.2)\n",
    "\n",
    "* The set of reference objects is critical, as the label transfer to the survey objects can only be as good as the quality of the reference label set.\n",
    "\n",
    "* 542 stars from 19 clusters \n",
    " * Clusters have an advantage over field stars: all stars have the same age, [Fe/H], distance, ...\n",
    "* Use known labels from APOGEE/ASPCAP pipeline ($T_{eff}$, log g, [$\\alpha$/Fe], [C/M], [N/M], micro-turbulence)\n",
    "* One goal is to place all stellar labels (true and inferred) on a common scale.\n",
    "\n",
    "__Figure 1 from Ness et al.:__ ASPCAP-corrected training set for The Cannon:\n",
    "<img src=\"figs/ness2015_fig1.jpg\" width=700 height=700></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Training Set\n",
    "\n",
    "* ASPCAP only includes stars with SNR>70 and log g < 3.5\n",
    " * No dwarf stars\n",
    "* Pleiades is the only cluster with dwarf stars (small training set may make it hard to assign dwarf star labels)\n",
    "* For clusters, stellar parameters should lie on single isochrone\n",
    "* \"Isochrone-corrected labels\": modify all training stars so their logg values agrees with with single Padova isochrone based on literature values of cluster age and metallicity\n",
    "\n",
    "__Figure 2 from Ness et al.:__ Isochrone-corrected training set for The Cannon:\n",
    "<img src=\"figs/ness2015_fig2.jpg\" width=700 height=700></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Continuum Normalization (Sec 2.3, 5.3)\n",
    "* In theory, continuum is defined as pixels that are not affected by any absoprtion or emission lines\n",
    "* In practice, very hard to find \"pure\" continuum in almost any spectrum\n",
    "* Good practice would be to find pixels that are not affected by changing any features in the model\n",
    "\n",
    "### In The Cannon:\n",
    "* First, define \"pseudo-continuum\" by using a polynomial to fit upper 90% of spectra as determined by a running median over 50A windows.  Effectively smooths out the spectrum.\n",
    " * Effective, but S/N dependent\n",
    "* Second, using the above as an initialization, run The Cannon on training set to find continuum pixels (Sec 5.3).\n",
    " * That is, using The Cannon, they find pixels that are minimally affected by changing stellar labels.\n",
    " * They find very small S/N dependence in continuum determination this way\n",
    "* Fit 2nd order [Chebyshev polynomials](http://mathworld.wolfram.com/ChebyshevPolynomialoftheFirstKind.html) to continuum pixels in hand for each of the 3 chips (15150-15800A, 15890-16430A, 16490-16950A)\n",
    " * Polynomials poorly contrained at boundaries\n",
    " \n",
    "__Figure 3 from Ness et al.:__ Example Continuum Normalized Spectra:\n",
    "<img src=\"figs/ness2015_fig3.jpg\" width=700 height=700></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.polynomial.Chebyshev.fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training the Generative Model (Sec 3)\n",
    "\n",
    "### Underlying assumptions:\n",
    " * Continuum-normalized spectra of stars with identical lablels look nearly-identical at every pixel\n",
    "  * Really an approximation since number of labels used is not exhaustive\n",
    " * Expected flux at every pixel changes continuously as a function of the labels\n",
    "* Model is generative: produces a PDF for the flux at every pixel/wavelength.\n",
    "\n",
    "### Define variable:\n",
    "* Number of reference stars: $N_{ref} = n$  \n",
    " * Each has continuum-normalized flux measurement $f_{n \\lambda}$ at each wavelength $\\lambda$\n",
    "* Each of the training spectra (of index $n$) has $K$ labels $\\ell_{nk}$\n",
    " * Assume labels are prefectly known.\n",
    "* label vector: $\\ell_n$\n",
    "* For any star, $n$ , at any pixel $\\lambda$, the flux $f_{n \\lambda}$ can be described as a smooth function of the star's labels: $\\ell(T_{eff}, \\log g, [Fe/H], \\cdots)$\n",
    "\n",
    "### Uncertainties\n",
    "* Observational uncertainties at each pixel: $\\sigma_{n \\lambda}$\n",
    "* Second noise term to account of possible systematics, other variations per pixel: $s_{n \\lambda}$\n",
    " * Presumed to be Gaussian\n",
    "\n",
    "### Write down model:\n",
    "\n",
    "* Predict the flux $f_{n \\lambda}$ at every pixel, given label and coefficient vectors: $\\ell_n$ and $\\theta_\\lambda$\n",
    "* Likelihood: $\\rm{ln} \\ p (f_n\\ | \\ l_n, \\theta) = \\sum_{\\lambda=1}^{L} \\rm{ln} \\ p(f_{n \\lambda} \\ |\\ \\ell_n, \\theta_{\\lambda}, s^2_{\\lambda})$\n",
    "\n",
    "* Single pixel Likelihood: $\\rm{ln}\\ p(f_{n \\lambda} \\ |\\ l_n, \\theta_{\\lambda}, s^2_{\\lambda}) = - \\frac{1}{2} \\frac{[f_{n \\lambda} - \\theta_{\\lambda}^{T} \\dot\\ \\ell_{n}]^2}{\\sigma_{n \\lambda}^2 + s_\\lambda^2}$ + $\\rm{ln}\\ (\\sigma_{n \\lambda}^2 + s_\\lambda^2)$\n",
    "\n",
    "* $\\ell^T \\equiv \\{1, T_{eff}, \\log g, [Fe/H], T_{eff}^2, T_{eff} \\log g, \\cdots, [Fe/H]^2\\}$\n",
    " * This vector contains permutations of stellar labels\n",
    " * Simplest model would be linear model (Eqn 5 in Ness et al.)\n",
    " * This is the quadratic model (Eqn 6 in Ness et al.)\n",
    "\n",
    "* $\\theta^T \\equiv \\{\\theta_{\\lambda}, s_{\\lambda}^2 \\}_{\\lambda=1}^{L}$\n",
    " * This vector is a vector of coefficients\n",
    " * Every pixel (i.e., every wavelength) has a set of coefficients\n",
    "\n",
    "* Some interpretation:\n",
    " * $\\theta_{\\lambda 0}$: baseline spectrum\n",
    " * $\\theta_{\\lambda k}$: first order coefficients are first derivatives of spectrum wrt to linear labels\n",
    " * $\\theta_{\\lambda k k^\\prime}$: second order coefficients are second derivatives of spectrum wrt to quadratic labels\n",
    " \n",
    "### Training the model\n",
    "\n",
    "* For training set $f_{n \\lambda}$ and $\\ell_n$ are known\n",
    "* Thus, can solve for $\\theta_{\\lambda}$ and $s_{\\lambda}$\n",
    "* Could use MCMC, but would be very slow ($\\sim$80,000 values of $\\theta_{\\lambda}$)\n",
    "* Use non-linear optimization (non-linear becuase of quadratic labels and noise $s_\\lambda^2$)\n",
    "* $\\theta_{\\lambda}, s_{\\lambda} \\leftarrow \\rm{argmax}(\\theta_{\\lambda}, s_{\\lambda}) \\sum_{n=1}^{N} \\ \\rm{ln} \\ p(f_{n, \\lambda} \\ | \\ \\theta_{\\lambda}, \\ell_{\\lambda}, s_{\\lambda}^2)$\n",
    "* Consider all pixels in survey, one pixel at a time\n",
    "\n",
    "\n",
    "### A very simple conceptual example\n",
    "\n",
    "* Suppose we only had two labels: $T_{\\rm eff}$ and [Fe/H] \n",
    "* A quadratic model for the flux at one pixel would be:\n",
    " - $f$ = $\\theta_0$ + $\\theta_1$ $T_{\\rm eff}$ + $\\theta_2$ [Fe/H] + $\\theta_3$ $T_{\\rm eff}$[Fe/H] + $\\theta_4$ $T_{\\rm eff}^2$ + $\\theta_5$ [Fe/H]$^2$\n",
    " - repeat procedure summing over all pixels\n",
    " - use the training data to find values for $\\theta_k$ (i.e., you have labels, solve for $\\theta_k$)\n",
    " - for validation/real data: you know each $\\theta_k$ and you want to assign a lablel for each star\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example non-linear optimization routines in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "curve_fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import polynomial as P\n",
    "P.polyfit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Labeling Survey Spectra (Sec 4)\n",
    "\n",
    "* \"Test data\": $M=m$ unlabeled spectra\n",
    " * continuum-normalized flux $f_{m \\lambda}$ at each wavelength $\\lambda$\n",
    " * associated uncertainty: $\\sigma_{m \\lambda}$\n",
    "\n",
    "* Assume spectral model coefficients from Step 3 and solve for labels of test data\n",
    "\n",
    "* $\\{\\ell_{m k}\\} \\leftarrow (\\rm{argmax})\\{\\ell_{m k} \\} \\sum_{\\lambda=1}^{N_{pix}} \\ \\rm{ln} \\ p(f_{m, \\lambda} \\ | \\ \\theta_{\\lambda}, \\ell_{m}, s_{\\lambda}^2)$\n",
    "\n",
    "* Follow same optimization procedure as above, only solving different variables\n",
    "* Optimize over all spectral model coefficients and scatter, considering all reference objects one pixel at a time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Results and Validation\n",
    "\n",
    "### Model Selection (Sec 5.1)\n",
    "\n",
    "* What about only using a linear-in-labels model?  Turns out to be too inflexible.\n",
    " * Show large and systematic deviations when run on objects with known labels\n",
    " * Next simplest is the quadratic model.\n",
    " * Is a quadratic model good enough? Beyond scope of paper.\n",
    " \n",
    "### Take-one-out Validation (Sec 5.2)\n",
    "\n",
    "* Use N-1 reference stars to train model\n",
    "* Run model on Nth star to predict its labels\n",
    "* Repeat $N_{ref}$ times\n",
    "* Only consider 3 labels ($T_{eff}, \\log g, [Fe/H]$)\n",
    "\n",
    "__Figure 4 from Ness et al.:__ Take-one-out Validation:\n",
    "<img src=\"figs/ness2015_fig4.jpg\" width=700 height=700></img>\n",
    "\n",
    "* The Cannon generally has smaller scatter than ASPCAP\n",
    "* They proceed to discuss trends and outliers in above plot.\n",
    "\n",
    "### Asessment of model performace (Sec 5.2)\n",
    "\n",
    "* Examine coefficients and scatter for example spectral regions A & B\n",
    "\n",
    "__Figure 5 from Ness et al.:__ Detailed look at one star:\n",
    "<img src=\"figs/ness2015_fig5.jpg\" width=700 height=700></img>\n",
    "\n",
    "\"Figure 5. First-order coefficients and scatter across the sample regions of the spectra from Figure 3, (A) and (B). Top panel: the baseline spectra representing the first coefficient from the set of reference spectra; middle panel: the next three coefficients (${\\theta }_{1}$, ${\\theta }_{2}$, ${\\theta }_{3}$), which correspond to the labels (${T}_{\\mathrm{eff}},\\mathrm{log}\\;g,[\\mathrm{Fe}/{\\rm{H}}]$ ); bottom panel: the scatter of the fit with a tenfold expanded vertical scale. The red, blue, and green areas in the top panel encompass the wavelength regions with the 5% highest (absolute value) coefficients for the $[\\mathrm{Fe}/{\\rm{H}}]$, $\\mathrm{log}\\;g$ and ${T}_{\\mathrm{eff}}$ labels, respectively. The ${T}_{\\mathrm{eff}}$ coefficient has been multiplied by a factor of 1000 simply to show this coefficient on a similar scale to the other coefficients. This indicates where the flux in these spectrum is particularly sensitive to the labels. Note that the $[\\mathrm{Fe}/{\\rm{H}}]$ label is dominant in the contribution level and from the top panel it is clear that there is significant covariance between the labels and there are only a few regions of $\\mathrm{log}\\;g$ sensitivity. The filled dots in the baseline spectrum in the top penal indicate the wavelengths at which the dependencies on all labels are weak, which we operatively identify as continuum pixels.\"\n",
    "\n",
    "* There are very few regions where the flux is a function of only one of the labels, and pixels are typically co-variant. (that is, the same pixel will have a higher flux at both lower ${T}_{\\mathrm{eff}}$ and higher $[\\mathrm{Fe}/{\\rm{H}}]$). This simply reflects well-known co-variances between, for example, temperature and $[\\mathrm{Fe}/{\\rm{H}}]$. The strongest $\\mathrm{log}\\;g$ dependence is typically associated with weak lines including the wings of the feature and the $[\\mathrm{Fe}/{\\rm{H}}]$ label, with strong lines, particularly the depth of the line.\n",
    "\n",
    "* The scatter is small and this indicates that our model is a good representation of the data. However, the scatter is highest where the most information in the spectra are contained. This implies that either our quadratic-in-labels spectral model is still somewhat too restricted, or that the labels of our training data set are imperfect or incomplete (for example, lacking $[\\alpha /\\mathrm{Fe}]$ as a label), or a combination of these effects.\n",
    "\n",
    "\n",
    "### Model-Data comparison (Sec 5.2)\n",
    "\n",
    "__Figure 6 from Ness et al.:__ Models + Scatter (cyan), data (black) for 4 stars:\n",
    "<img src=\"figs/ness2015_fig6.jpg\" width=1500 height=1500></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Application to APOGEE (Sec 5.4)\n",
    "\n",
    "__Figure 7 from Ness et al.:__ Application to APOGEE:\n",
    "<img src=\"figs/ness2015_fig7.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 7. ASPCAP DR10 vs. The Cannon for six different fields including in the disk, bulge, and halo. The number of stars for each subfigure is 211 (4431), 207 (4384), 217 (4399), 210 (4309), 198 (4311), 319 (4255). Each panel lists the mean difference between the labels (bias), the scatter between the labels (rms), and the formal uncertainly returned by The Cannon (precision).\n",
    "\n",
    "* There are weak trends; at low ${T}_{\\mathrm{eff}}$ ~ 3700 K, we find temperatures about 100 K cooler than APOGEE and at low $\\mathrm{log}\\;g$ we find ~0.15 dex larger $\\mathrm{log}\\;g$ than APOGEE. At the lowest metallicities $[\\mathrm{Fe}/{\\rm{H}}]\\;\\lt \\;-2.0$, we typically report higher metallicities on the order of 0.05 to 0.3 dex\n",
    "\n",
    "__Figure 8 from Ness et al.:__ Assessment of Biases:\n",
    "<img src=\"figs/ness2015_fig8.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 8. Difference between the labels (${T}_{\\mathrm{eff}}$, $\\mathrm{log}\\;g$, and $[\\mathrm{Fe}/{\\rm{H}}]$) derived by The Cannon and their ASPCAP DR10 values for all the 1400 stars shown in Figure 7. The error bars are dominated by those quoted by ASPCAP. There are systematic offsets at the coolest temperatures.\n",
    "\n",
    "### Comparison to isochrones\n",
    "\n",
    "__Figure 9 from Ness et al.:__ Comparison to expectations from isochrones:\n",
    "<img src=\"figs/ness2015_fig9.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 9. Labels for the ~35,000 stars from DR10 derived by The Cannon based on ASPCAP-corrected labels for the set of reference objects. The set of panels on the left shows ${T}_{\\mathrm{eff}}$–$\\mathrm{log}\\;g$ in four metallicity bins. There are ~19,000, 13,000, 1600, and 1000 stars in the most metal-rich to metal-poor metallicity bins, respectively. The isochrones plotted are 10 Gyr Padova isochrones at the metallicities marked in the upper left hand corners of each sub-panel. The panel on the right shows all stars colored in $[\\mathrm{Fe}/{\\rm{H}}]$ on the four isochrones. Note that the $\\mathrm{log}\\;g$ distribution at low $\\mathrm{log}\\;g$ is narrow and offset from the giant branch. Reference objects are shown as open circles.\n",
    "\n",
    "### Scatter in the labels\n",
    "\n",
    "* Rerun model with 20 bootstrap realizations of training set to quantify formal scatter in labels\n",
    "* Scatter largest in regions outside training set\n",
    "\n",
    "\n",
    "__Figure 11 from Ness et al.:__ Formal Scatter in Labels:\n",
    "<img src=\"figs/ness2015_fig11.jpg\" width=700 height=700></img>\n",
    "\n",
    "Figure 11. Standard deviation in the labels returned in ${T}_{\\mathrm{eff}}$, $\\mathrm{log}\\;g$ and $[\\mathrm{Fe}/{\\rm{H}}]$, shown in the ${T}_{\\mathrm{eff}}$−$\\mathrm{log}\\;g$ plane, normalized by the optimization error on each measurement, for 20 bootstrapping tests of the training set. The representative sample of ~670 stars shown here has been drawn from an equal sampling of a grid spaced by 100 K in ${T}_{\\mathrm{eff}}$, 0.25 dex in $\\mathrm{log}\\;g$ and 0.25 dex in $[\\mathrm{Fe}/{\\rm{H}}]$ from the labels returned using the model trained on the isochrone-corrected reference objects. The location of the reference objects is shown in the gray shaded regions in the panel. Note the narrow region of reference objects also on the main sequence. The highest scatter in the labels is seen for regions where the labels are extrapolated. These figures are shown for the isochrone-corrected labels discussed in Section 2.4.\n",
    "\n",
    "\n",
    "\n",
    "### How well does extrapolation work?\n",
    "\n",
    "__Figure 12 from Ness et al.:__ Formal Scatter in Labels:\n",
    "<img src=\"figs/ness2015_fig12.jpg\" width=700 height=700></img>\n",
    "\n",
    "\n",
    "Figure 12. Difference in labels between The Cannon and ASPCAP indicating the regions of extrapolation where the difference in the labels extends beyond the estimated errors of the methods, due to the limited sampling of the reference objects which does not fully cover the label-space of the survey. The ASPCAP-corrected training labels were used to generate the model applied at the test step on the DR10 data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
