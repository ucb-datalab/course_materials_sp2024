{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ../talktools.py\n",
    "!rm galaxy1000.csv\n",
    "!rm test.csv\n",
    "!rm -r joblib_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A (Simplistic) Introduction to Data Caching\n",
    "\n",
    "<div style=\"text-align: center\"><font size=-1>AY128/256 UC Berkeley (2024)</font> </div> \n",
    "\n",
    "<quote>\n",
    "    <i><font color=\"grey\">\"There are two hard things in Computer Science, cache invalidation, naming things, and off-by-one errors.\"</i></font>\n",
    "    <div style=\"text-align: right\"><font color=\"grey\">--Socrates</font></div>\n",
    "\n",
    "</quote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Motivation</b>: In data-intensive workloads, like the ones you'll be building in this class, it's a good idea to think about your data caching strategies up front. During exploratory analysis, especially when interacting with external sources of databases (e.g., flat files hosted remotely, external databases, etc.) sensible caching can save you a bunch of time in not needing to (re)download data. \n",
    "\n",
    "Even for pure local interactions, you might want to cache the results of computation to avoid having to rerun long-running/expensive calcuations.\n",
    "\n",
    "In the context of reproducbility/replicability, having a cache/store of the data inputs will make it easier for others to redo the steps you did and get the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent or Ephemeral?\n",
    "\n",
    "Every time you run an expensive computation or do a time-intensive query, you implicitly store the results in emphemeral Python object.\n",
    "\n",
    "Let's get a CSV file with some data about galaxies from SDSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import requests  # requests is the standard URL\n",
    "\n",
    "external_file_location = \"https://raw.githubusercontent.com/AstroHackWeek/AstroHackWeek2014/master/day4/galaxy1000.csv\"\n",
    "local_filename = os.path.basename(external_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ATTEMPT 1: download the CSV and load it into a DataFrame\n",
    "r = requests.get(external_file_location)\n",
    "df = pd.read_csv(io.StringIO(r.text))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great: we can use the pandas DataFrame `df` in the downstream analysis. The **problem of course is that we rerun our notebook, we need to redownload the data**. This ephemeral caching is usually just fine when the data is small and the computations are quick, but generally slows us down with larger files and beefier computations. \n",
    "\n",
    "Why don't we modify the above to check to see if the file exists and load it in if so. That is, let's create a persistent store of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_galaxy_dataframe():\n",
    "    \"\"\"\n",
    "    get the galaxy CSV file from SDSS if we dont have it already\n",
    "    and parse it into a pandas DataFrame. Save the file\n",
    "    for future use.\n",
    "    \n",
    "    Returns: dataframe\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_filename):\n",
    "        start = time.time()\n",
    "        r = requests.get(external_file_location)\n",
    "\n",
    "        # note: below is for smallish files. To download and save larger files\n",
    "        # see https://stackoverflow.com/a/14114741\n",
    "        with open(local_filename, 'w') as handle:\n",
    "            handle.write(r.text)\n",
    "\n",
    "        print(f\"Wrote the file {local_filename} to disk\")\n",
    "        # from the Python object `r` instead of from disk to avoid disk IO\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    else:\n",
    "        start = time.time()\n",
    "        print(f\"Reading the file {local_filename} from disk\")\n",
    "        df = pd.read_csv(local_filename)\n",
    "        print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!rm galaxy1000.csv\n",
    "get_galaxy_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, the capability (download and save, otherwise load) might be something we want to use a lot. Let's make the function a big more generic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_and_parse_cached_remote_csvfile(url, local_filename=None, verbose=True):\n",
    "    \"\"\"\n",
    "    get a CSV file at `url` if we dont have it already\n",
    "    and parse it into a pandas DataFrame. Save the file\n",
    "    for future use. Guess the output filename if not given.\n",
    "    \n",
    "    Returns: dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # here we might error check to see if the URL is valid, points to a CSV file\n",
    "    # etc.\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "        \n",
    "    if not os.path.exists(local_filename):\n",
    "        start = time.time()\n",
    "        r = requests.get(url)\n",
    "\n",
    "        # note: below is for smallish files. To download and save larger files\n",
    "        # see https://stackoverflow.com/a/14114741\n",
    "        with open(local_filename, 'w') as handle:\n",
    "            handle.write(r.text)\n",
    "        if verbose:\n",
    "            print(f\"Wrote the file {local_filename} to disk\", flush=True)\n",
    "        # from the Python object `r` instead of from disk to avoid disk IO\n",
    "        df = pd.read_csv(io.StringIO(r.text))\n",
    "        if verbose:\n",
    "            print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    else:\n",
    "        start = time.time()\n",
    "        if verbose:\n",
    "            print(f\"Reading the file {local_filename} from disk\")\n",
    "        df = pd.read_csv(local_filename)\n",
    "        if verbose:\n",
    "            print(f\"  Total time: {time.time() - start:0.3} sec\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_and_parse_cached_remote_csvfile(external_file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_and_parse_cached_remote_csvfile(external_file_location, local_filename=\"test.csv\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a similar workflow for external queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.gaia import Gaia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\" -- Get some nearby sources\n",
    "        SELECT top 1000 \n",
    "            ra,dec,source_id,parallax,pm\n",
    "        FROM gaiaedr3.gaia_source \n",
    "        WHERE parallax_over_error > 15.0\n",
    "            AND parallax > 200.0\n",
    "        ORDER BY parallax DESC\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_gaia_query(q):\n",
    "    start = time.time()\n",
    "    job = Gaia.launch_job(q)\n",
    "    print(f\"Total time: {time.time()-start:0.2f} sec\")\n",
    "    return job.get_results()\n",
    "\n",
    "result_table = get_gaia_query(query)\n",
    "print(result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great: we can use `result_table` in the downstream analysis. The problem of course is that we rerun our notebook we have to download the data again (and the computation on the remote side needs to happen as well. This isn't ecofriendly!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be better off memorizing the result for a given input and save the data locally. We could do this \"by hand\" but [`joblib`](https://joblib.readthedocs.io/en/latest/) was built for just this (and parallel computing):\n",
    "\n",
    "<pre>\n",
    "    The Memory class defines a context for lazy evaluation of function, by putting the results in a store, by default using a disk, and not re-running the function twice for the same arguments. It works by explicitly saving the output to a file and it is designed to work with non-hashable and potentially large input and output data types such as numpy arrays.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "cachedir = './joblib_cache'\n",
    "memory = Memory(cachedir, verbose=0, bytes_limit=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_gaia_query(q):\n",
    "    start = time.time()\n",
    "    job = Gaia.launch_job(q)\n",
    "    print(f\"Total time: {time.time()-start:0.2f} sec\")\n",
    "    return job.get_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `memory` as a function decorator. Every time `get_gaia_query` is called now, the variable `q` will be checked. If we've seen this before, then joblib will just return the output from last time. Otherwise we'll run this function and save the results in the `joblib_cache` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_table = get_gaia_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time result_table = get_gaia_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+1 color=red>**Question**</font>: In what case's might you NOT want to cache query results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_weather_alerts(area=\"CA\"):\n",
    "    r = requests.get(f\"https://api.weather.gov/alerts/active?area={area}\")\n",
    "    zone = r.json()[\"features\"][0][\"properties\"][\"areaDesc\"]\n",
    "    return f'{zone}: {r.json()[\"features\"][0][\"properties\"][\"headline\"]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(get_weather_alerts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
