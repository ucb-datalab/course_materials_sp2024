{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC Convergance \n",
    "\n",
    "### AY 128/256 (UC Berkeley, 2021-2024)\n",
    "\n",
    "* What is convergence for MCMC?\n",
    " - Working definition: The property that the chain is in equilbrium, stationary, and/or not sensitive to initial conditions.\n",
    " - __Convergence can never be proven__\n",
    " - Our best bet is to learn to look for situations when chain hasn't coverned, and to get in the habit of carefully checking the results of any model fitting\n",
    "\n",
    "#### Sampling from a 1-D Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "%matplotlib inline\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from arviz import plot_trace as traceplot\n",
    "\n",
    "# set up a pymc model to sample from a 1-D Gaussian with mean=0 and sigma=1.\n",
    "with pm.Model() as model:\n",
    "    mean, sigma = 0, 1\n",
    "    vals = pm.Normal('x', mu=mean, sigma=sigma)\n",
    "    trace = pm.sample(1000, tune=50, chains=2, cores=2, discard_tuned_samples=True)#, return_inferencedata=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print pymc summary statistics\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make trace plots\n",
    "with model:\n",
    "    traceplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a (pointless) corner plot\n",
    "import corner\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "_ = corner.corner(trace, quantiles=[0.16, 0.84], truths=[mean], labels=['mean'], fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have samples from the posterior we can do cool things:\n",
    "\n",
    "Suppose we have B samples $\\theta_1$...$\\theta_B$ from the posterior $p(\\theta|$**X**):\n",
    "\n",
    "1) **Posterior mean**: \n",
    "   \n",
    "The exact equation $E[\\theta|$**X**] = $\\int \\theta~ p(\\theta|$**X**)$~d\\theta$\n",
    "\n",
    "Using the sample $E[\\theta|$**X**] $\\approx \\frac{1}{B} \\sum_{b=1}^B \\theta_b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Marginalization**: \n",
    "   \n",
    "The exact equation $p(\\theta_1|$**X**) = $\\int p(\\theta_1,\\theta_2,..\\theta_p|$**X**)$~d\\theta_2~d\\theta_3...d\\theta_p$\n",
    "\n",
    "Using the sample $p(\\theta_1|$**X**) $\\sim \\theta_{1,1} ... \\theta_{1,B}$\n",
    "\n",
    "*That is, record the parameter of interest $\\theta_1$ from each sample.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Prediction**: \n",
    "   \n",
    "The exact equation $p(\\tilde{X}|$**X**) = $\\int p(\\tilde{X}|\\theta)~p(\\theta|$**X**)$~d\\theta$\n",
    "\n",
    "Using the sample $p(\\tilde{X}|$**X**) $\\sim \\tilde{x_1} | \\theta_{1} ... \\tilde{x_B} | \\theta_{B}$\n",
    "\n",
    "*That is, take each sample of $\\theta$ and determine a value for $x$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general advice for checking goodness of fit:\n",
    "\n",
    "Determining whether an MCMC has converged can be difficult, especially in high-dimensional parameter spaces: \"This can be a difficult subject to discuss because it isn’t formally possible to guarantee convergence for any but the simplest models, and therefore any argument that you make will be circular and heuristic.\"\n",
    "\n",
    "Here's a basic overview of how you can convince yourself and others you are on the right track.\n",
    "\n",
    "- **First check** - start multiple chains from different starting values and see that they converge to the same place\n",
    "- **More formal methods** - Raftery-Lewis, Geweke, autocorrelation, etc.\n",
    "- **Goodness of fit** - Posterior Predictive Checks which simulate data from your fitted model and compare to the observed data (checks convergence AND the suitability of the chosen model)\n",
    "\n",
    "See https://pkgw.github.io/mcmc-reporting/ and https://rlhick.people.wm.edu/stories/bayesian_5.html for some details and insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "https://pymcmc.readthedocs.io/en/latest/modelchecking.html\n",
    "\n",
    "A number of diagnostics (both formal and informal) exist:\n",
    "\n",
    "- Geweke score: compares mean of beginning of chain with mean of end\n",
    "\n",
    "Geweke score = $\\frac{\\bar{\\theta}_e - \\bar{\\theta}_b}{\\sqrt{Var(\\theta_e) + Var(\\theta_b)}}$\n",
    "\n",
    "- Gelman-Rubin: compare variance between chains to variance of single chain.  For a well converged chain the G-R stat should approach 1. Values greater than typically 1.1 indicate that the chains have not yet fully converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from arviz.stats import geweke\n",
    "from arviz.stats import rhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance Fraction \n",
    "\n",
    "This is fraction of proposed steps were accepted.\n",
    "\n",
    "From https://pkgw.github.io/mcmc-reporting/ : you should report the \"jump acceptance fractions computed for each chain, or a summary of them if there are many chains and/or parameters. Acceptance fractions outside the range of 10–90% suggest that the sampler is not well-matched to your problem and are cause for concern, since your samples may not be fully exploring the posterior distribution.\"\n",
    "\n",
    "For pymc/NUTS, it gives you a warning if you miss the target acceptance fraction.\n",
    "\n",
    "Why might an acceptance fraction too high/low cause problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "\n",
    "A good heuristic for assessing convergence of samplings is the integrated autocorrelation time.  The integrated autocorrelation time quantifies \"the effects of sampling error on your results. The basic idea is that the samples in your chain are not independent and you must estimate the effective number of independent samples.\" (https://emcee.readthedocs.io/en/latest/tutorials/autocorr/#autocorr). \n",
    "\n",
    "This statistic makes more sense for a M-H or similar type of non-adaptive sampler.\n",
    "\n",
    "See also [these lecture notes](https://pdfs.semanticscholar.org/0bfe/9e3db30605fe2d4d26e1a288a5e2997e7225.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make an autocorrelation plot easily in PyMC3\n",
    "az.plot_autocorr(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise for the reader -- check out correlation statistics with a multi-dimensional Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this setups up sampling for a multi-dimensional Gaussian for you to play around with\n",
    "with pm.Model() as model:\n",
    "    ndim = 2\n",
    "    np.random.seed(42)\n",
    "    means = np.random.rand(ndim)\n",
    "\n",
    "    cov = 0.5 - np.random.rand(ndim ** 2).reshape((ndim, ndim))\n",
    "    cov = np.triu(cov)\n",
    "    cov += cov.T - np.diag(cov.diagonal())\n",
    "    cov = np.dot(cov, cov)\n",
    "    \n",
    "    vals = pm.MvNormal('vals', mu=means, cov=cov, shape=(2, 2))\n",
    "    \n",
    "\n",
    "    trace = pm.sample(draws=1000, tune=100, chains=2, cores=2, discard_tuned_samples=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(means)\n",
    "az.summary(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceplot(trace)\n",
    "\n",
    "samples = pm.trace_to_dataframe(trace)\n",
    "#_ = corner.corner(samples, quantiles=[0.16, 0.84], truths=[means[0], means[1], means[0], means[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the autocorrelation plots for each chain for each variable\n",
    "az.plot_autocorr(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
